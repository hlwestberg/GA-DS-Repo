{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Text Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import sklearn\n",
    "sklearn.set_config(\"display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/SMSSpamCollection.txt', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['spam', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   spam                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in this case, we only have text to predict if a message is spam\n",
    "- the binary bag of words takes all of these words and splits them out into a group\n",
    "- we're basically destroying the order of the text, just looking at the words as independent entities\n",
    "- then we create a matrix oc 0/1 to denote of a specific word is in a phrase\n",
    "- so we take the column of text and transform it so that it will look like a 0/1 matrix that becomes our X, which we can use to fit a model with X and Y.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Bag of Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvect= CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## usually we would put double brackets around X, but we don't need to do that here\n",
    "## because the count vectorizer expects one column\n",
    "X = df['text']\n",
    "y = df['spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test, = train_test_split(X, y, random_state=42, stratify =y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4747           Orh i tot u say she now still dun believe.\n",
       "5295    Alex says he's not ok with you not being ok wi...\n",
       "5568                 Will Ã¼ b going to esplanade fr home?\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4179x7387 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 55514 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvect.fit_transform(X_train)\n",
    "## this message is giving you a condensed version, since it's really wide (sparse) and has\n",
    "## alot of zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorized = cvect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to convert it back to a dense matrix/array, do this:\n",
    "# only using train data bc at this point we wouldn't know the test data\n",
    "X_train_vectorized.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '000pes',\n",
       " '008704050406',\n",
       " '0089',\n",
       " '0121',\n",
       " '01223585236',\n",
       " '01223585334',\n",
       " '02',\n",
       " '0207',\n",
       " '02072069400',\n",
       " '02073162414',\n",
       " '02085076972',\n",
       " '021',\n",
       " '03',\n",
       " '04',\n",
       " '0430',\n",
       " '05',\n",
       " '050703',\n",
       " '0578',\n",
       " '06',\n",
       " '07',\n",
       " '07008009200',\n",
       " '07046744435',\n",
       " '07099833605',\n",
       " '07123456789',\n",
       " '0721072',\n",
       " '07732584351',\n",
       " '07734396839',\n",
       " '07742676969',\n",
       " '07753741225',\n",
       " '07781482378',\n",
       " '07786200117',\n",
       " '077xxx',\n",
       " '078',\n",
       " '07801543489',\n",
       " '07808',\n",
       " '07808247860',\n",
       " '07808726822',\n",
       " '07815296484',\n",
       " '07821230901',\n",
       " '078498',\n",
       " '0789xxxxxxx',\n",
       " '0796xxxxxx',\n",
       " '07xxxxxxxxx',\n",
       " '0800',\n",
       " '08000407165',\n",
       " '08000839402',\n",
       " '08000930705',\n",
       " '08000938767',\n",
       " '08001950382',\n",
       " '08002888812',\n",
       " '08002986030',\n",
       " '08002986906',\n",
       " '08002988890',\n",
       " '08006344447',\n",
       " '0808',\n",
       " '08081263000',\n",
       " '08081560665',\n",
       " '0825',\n",
       " '083',\n",
       " '0844',\n",
       " '08448350055',\n",
       " '08448714184',\n",
       " '0845',\n",
       " '08450542832',\n",
       " '08452810073',\n",
       " '08452810075over18',\n",
       " '0870',\n",
       " '08700469649',\n",
       " '08700621170150p',\n",
       " '08701213186',\n",
       " '08701417012',\n",
       " '08701417012150p',\n",
       " '0870141701216',\n",
       " '087018728737',\n",
       " '0870241182716',\n",
       " '08702840625',\n",
       " '08706091795',\n",
       " '0870737910216yrs',\n",
       " '08707500020',\n",
       " '08707509020',\n",
       " '0870753331018',\n",
       " '08707808226',\n",
       " '08708034412',\n",
       " '08709222922',\n",
       " '08709501522',\n",
       " '0871',\n",
       " '087104711148',\n",
       " '08712101358',\n",
       " '0871212025016',\n",
       " '08712300220',\n",
       " '087123002209am',\n",
       " '08712317606',\n",
       " '08712400200',\n",
       " '08712400602450p',\n",
       " '08712402050',\n",
       " '08712402578',\n",
       " '08712402779',\n",
       " '08712402902',\n",
       " '08712404000',\n",
       " '08712405020',\n",
       " '08712405022',\n",
       " '08712460324',\n",
       " '08712466669',\n",
       " '0871277810710p',\n",
       " '0871277810910p',\n",
       " '08714342399',\n",
       " '087147123779am',\n",
       " '08714712388',\n",
       " '08714712412',\n",
       " '08714714011',\n",
       " '08715203028',\n",
       " '08715203652',\n",
       " '08715203677',\n",
       " '08715203685',\n",
       " '08715705022',\n",
       " '08717111821',\n",
       " '08717168528',\n",
       " '08717205546',\n",
       " '0871750',\n",
       " '08717509990',\n",
       " '08717898035',\n",
       " '08718711108',\n",
       " '08718720201',\n",
       " '08718723815',\n",
       " '08718725756',\n",
       " '08718726270',\n",
       " '087187262701',\n",
       " '08718726970',\n",
       " '08718726971',\n",
       " '08718726978',\n",
       " '087187272008',\n",
       " '08718727870',\n",
       " '08718727870150ppm',\n",
       " '08718730666',\n",
       " '08718738001',\n",
       " '08718738002',\n",
       " '08718738034',\n",
       " '08719180219',\n",
       " '08719180248',\n",
       " '08719181259',\n",
       " '08719181503',\n",
       " '08719839835',\n",
       " '08719899217',\n",
       " '08719899229',\n",
       " '08719899230',\n",
       " '09',\n",
       " '09041940223',\n",
       " '09050000301',\n",
       " '09050000332',\n",
       " '09050000460',\n",
       " '09050000555',\n",
       " '09050000878',\n",
       " '09050000928',\n",
       " '09050001808',\n",
       " '09050003091',\n",
       " '09050090044',\n",
       " '09050280520',\n",
       " '09053750005',\n",
       " '09056242159',\n",
       " '09057039994',\n",
       " '09058091854',\n",
       " '09058091870',\n",
       " '09058094455',\n",
       " '09058094507',\n",
       " '09058094565',\n",
       " '09058094583',\n",
       " '09058094594',\n",
       " '09058094597',\n",
       " '09058094599',\n",
       " '09058095107',\n",
       " '09058097189',\n",
       " '09058097218',\n",
       " '09058098002',\n",
       " '09058099801',\n",
       " '09061104276',\n",
       " '09061104283',\n",
       " '09061209465',\n",
       " '09061213237',\n",
       " '09061221061',\n",
       " '09061221066',\n",
       " '09061701461',\n",
       " '09061701851',\n",
       " '09061701939',\n",
       " '09061702893',\n",
       " '09061743386',\n",
       " '09061743806',\n",
       " '09061743811',\n",
       " '09061744553',\n",
       " '09061790121',\n",
       " '09061790125',\n",
       " '09063440451',\n",
       " '09063442151',\n",
       " '09063458130',\n",
       " '0906346330',\n",
       " '09064011000',\n",
       " '09064012103',\n",
       " '09064012160',\n",
       " '09064015307',\n",
       " '09064017295',\n",
       " '09064017305',\n",
       " '09064018838',\n",
       " '09064019014',\n",
       " '09064019788',\n",
       " '09065069120',\n",
       " '09065069154',\n",
       " '09065171142',\n",
       " '09065174042',\n",
       " '09065394973',\n",
       " '09065989180',\n",
       " '09065989182',\n",
       " '09066350750',\n",
       " '09066358152',\n",
       " '09066358361',\n",
       " '09066362206',\n",
       " '09066362231',\n",
       " '09066364311',\n",
       " '09066364349',\n",
       " '09066364589',\n",
       " '09066368327',\n",
       " '09066368470',\n",
       " '09066368753',\n",
       " '09066380611',\n",
       " '09066382422',\n",
       " '09066612661',\n",
       " '09066649731from',\n",
       " '09066660100',\n",
       " '09071512432',\n",
       " '09071512433',\n",
       " '09077818151',\n",
       " '09090900040',\n",
       " '09094100151',\n",
       " '09094646899',\n",
       " '09095350301',\n",
       " '09099725823',\n",
       " '09099726395',\n",
       " '09099726481',\n",
       " '09099726553',\n",
       " '09111030116',\n",
       " '09701213186',\n",
       " '0quit',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '1000call',\n",
       " '1000s',\n",
       " '100percent',\n",
       " '100txt',\n",
       " '1013',\n",
       " '1030',\n",
       " '10am',\n",
       " '10k',\n",
       " '10p',\n",
       " '10ppm',\n",
       " '10th',\n",
       " '11',\n",
       " '1120',\n",
       " '113',\n",
       " '1131',\n",
       " '114',\n",
       " '116',\n",
       " '11mths',\n",
       " '11pm',\n",
       " '12',\n",
       " '1205',\n",
       " '120p',\n",
       " '121',\n",
       " '1225',\n",
       " '125',\n",
       " '1250',\n",
       " '125gift',\n",
       " '128',\n",
       " '12hours',\n",
       " '12hrs',\n",
       " '12mths',\n",
       " '13',\n",
       " '130',\n",
       " '1327',\n",
       " '139',\n",
       " '14',\n",
       " '140',\n",
       " '140ppm',\n",
       " '145',\n",
       " '1450',\n",
       " '146tf150p',\n",
       " '14tcr',\n",
       " '14thmarch',\n",
       " '15',\n",
       " '150',\n",
       " '1500',\n",
       " '150p',\n",
       " '150p16',\n",
       " '150pm',\n",
       " '150ppermesssubscription',\n",
       " '150ppm',\n",
       " '150ppmsg',\n",
       " '150pw',\n",
       " '151',\n",
       " '153',\n",
       " '15pm',\n",
       " '16',\n",
       " '165',\n",
       " '169',\n",
       " '177',\n",
       " '18',\n",
       " '180',\n",
       " '18p',\n",
       " '18yrs',\n",
       " '1956669',\n",
       " '1apple',\n",
       " '1b6a5ecef91ff9',\n",
       " '1cup',\n",
       " '1da',\n",
       " '1er',\n",
       " '1hr',\n",
       " '1im',\n",
       " '1lemon',\n",
       " '1mega',\n",
       " '1million',\n",
       " '1pm',\n",
       " '1st',\n",
       " '1stchoice',\n",
       " '1stone',\n",
       " '1thing',\n",
       " '1tulsi',\n",
       " '1win150ppmx3',\n",
       " '1winaweek',\n",
       " '1x150p',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '2003',\n",
       " '2004',\n",
       " '2005',\n",
       " '2006',\n",
       " '2007',\n",
       " '2025050',\n",
       " '20m12aq',\n",
       " '20p',\n",
       " '21',\n",
       " '21870000',\n",
       " '21st',\n",
       " '22',\n",
       " '220',\n",
       " '2309',\n",
       " '24',\n",
       " '24hrs',\n",
       " '24m',\n",
       " '24th',\n",
       " '25',\n",
       " '250',\n",
       " '250k',\n",
       " '255',\n",
       " '25p',\n",
       " '26',\n",
       " '2667',\n",
       " '26th',\n",
       " '27',\n",
       " '28',\n",
       " '2814032',\n",
       " '28days',\n",
       " '28th',\n",
       " '29',\n",
       " '2b',\n",
       " '2bold',\n",
       " '2c',\n",
       " '2channel',\n",
       " '2day',\n",
       " '2docd',\n",
       " '2end',\n",
       " '2exit',\n",
       " '2ez',\n",
       " '2find',\n",
       " '2getha',\n",
       " '2geva',\n",
       " '2go',\n",
       " '2hook',\n",
       " '2hrs',\n",
       " '2i',\n",
       " '2lands',\n",
       " '2marrow',\n",
       " '2moro',\n",
       " '2morow',\n",
       " '2morro',\n",
       " '2morrow',\n",
       " '2morrowxxxx',\n",
       " '2mro',\n",
       " '2mrw',\n",
       " '2nd',\n",
       " '2nhite',\n",
       " '2nights',\n",
       " '2nite',\n",
       " '2optout',\n",
       " '2p',\n",
       " '2price',\n",
       " '2px',\n",
       " '2rcv',\n",
       " '2stop',\n",
       " '2stoptx',\n",
       " '2stoptxt',\n",
       " '2u',\n",
       " '2u2',\n",
       " '2watershd',\n",
       " '2waxsto',\n",
       " '2wks',\n",
       " '2wt',\n",
       " '2wu',\n",
       " '2years',\n",
       " '2yr',\n",
       " '30',\n",
       " '300',\n",
       " '3000',\n",
       " '300603',\n",
       " '300603t',\n",
       " '300p',\n",
       " '3030',\n",
       " '30apr',\n",
       " '30ish',\n",
       " '30pp',\n",
       " '30s',\n",
       " '30th',\n",
       " '3100',\n",
       " '310303',\n",
       " '31p',\n",
       " '32',\n",
       " '32000',\n",
       " '3230',\n",
       " '32323',\n",
       " '326',\n",
       " '33',\n",
       " '330',\n",
       " '350',\n",
       " '3510i',\n",
       " '35p',\n",
       " '3650',\n",
       " '36504',\n",
       " '3680',\n",
       " '3750',\n",
       " '37819',\n",
       " '38',\n",
       " '382',\n",
       " '391784',\n",
       " '3aj',\n",
       " '3d',\n",
       " '3days',\n",
       " '3g',\n",
       " '3gbp',\n",
       " '3hrs',\n",
       " '3lions',\n",
       " '3lp',\n",
       " '3miles',\n",
       " '3mins',\n",
       " '3optical',\n",
       " '3pound',\n",
       " '3qxj9',\n",
       " '3rd',\n",
       " '3ss',\n",
       " '3uz',\n",
       " '3wks',\n",
       " '3x',\n",
       " '3xx',\n",
       " '40',\n",
       " '400',\n",
       " '400mins',\n",
       " '400thousad',\n",
       " '402',\n",
       " '4041',\n",
       " '40411',\n",
       " '40gb',\n",
       " '40mph',\n",
       " '41685',\n",
       " '41782',\n",
       " '420',\n",
       " '42478',\n",
       " '42810',\n",
       " '430',\n",
       " '434',\n",
       " '44',\n",
       " '440',\n",
       " '4403ldnw1a7rw18',\n",
       " '44345',\n",
       " '447797706009',\n",
       " '447801259231',\n",
       " '448712404000',\n",
       " '449050000301',\n",
       " '45',\n",
       " '450ppw',\n",
       " '45239',\n",
       " '45pm',\n",
       " '47',\n",
       " '4742',\n",
       " '47per',\n",
       " '48',\n",
       " '4882',\n",
       " '48922',\n",
       " '49',\n",
       " '49557',\n",
       " '4a',\n",
       " '4brekkie',\n",
       " '4d',\n",
       " '4eva',\n",
       " '4get',\n",
       " '4give',\n",
       " '4got',\n",
       " '4info',\n",
       " '4msgs',\n",
       " '4mths',\n",
       " '4my',\n",
       " '4t',\n",
       " '4th',\n",
       " '4the',\n",
       " '4thnov',\n",
       " '4txt',\n",
       " '4u',\n",
       " '4utxt',\n",
       " '4w',\n",
       " '4wrd',\n",
       " '4xx26',\n",
       " '4years',\n",
       " '50',\n",
       " '500',\n",
       " '5000',\n",
       " '505060',\n",
       " '50award',\n",
       " '50gbp',\n",
       " '50p',\n",
       " '50perwksub',\n",
       " '50pm',\n",
       " '50pmmorefrommobile2bremoved',\n",
       " '50ppm',\n",
       " '50rcvd',\n",
       " '50s',\n",
       " '5226',\n",
       " '5249',\n",
       " '526',\n",
       " '530',\n",
       " '54',\n",
       " '542',\n",
       " '5digital',\n",
       " '5free',\n",
       " '5ish',\n",
       " '5k',\n",
       " '5min',\n",
       " '5mls',\n",
       " '5p',\n",
       " '5pm',\n",
       " '5th',\n",
       " '5wb',\n",
       " '5we',\n",
       " '5wkg',\n",
       " '5wq',\n",
       " '5years',\n",
       " '60',\n",
       " '600',\n",
       " '6031',\n",
       " '6089',\n",
       " '60p',\n",
       " '61',\n",
       " '61200',\n",
       " '61610',\n",
       " '6230',\n",
       " '62468',\n",
       " '630',\n",
       " '63miles',\n",
       " '645',\n",
       " '65',\n",
       " '650',\n",
       " '66',\n",
       " '674',\n",
       " '67441233',\n",
       " '68866',\n",
       " '69101',\n",
       " '69200',\n",
       " '69669',\n",
       " '69696',\n",
       " '69855',\n",
       " '69866',\n",
       " '69876',\n",
       " '69888',\n",
       " '69888nyt',\n",
       " '69969',\n",
       " '69988',\n",
       " '6days',\n",
       " '6hrs',\n",
       " '6months',\n",
       " '6ph',\n",
       " '6pm',\n",
       " '6th',\n",
       " '6times',\n",
       " '6wu',\n",
       " '700',\n",
       " '71',\n",
       " '7250',\n",
       " '7250i',\n",
       " '730',\n",
       " '74355',\n",
       " '75',\n",
       " '750',\n",
       " '7548',\n",
       " '75max',\n",
       " '77',\n",
       " '7732584351',\n",
       " '78',\n",
       " '786',\n",
       " '7876150ppm',\n",
       " '7am',\n",
       " '7ish',\n",
       " '7mp',\n",
       " '7oz',\n",
       " '7pm',\n",
       " '7th',\n",
       " '80',\n",
       " '800',\n",
       " '8000930705',\n",
       " '80062',\n",
       " '8007',\n",
       " '80082',\n",
       " '80086',\n",
       " '80155',\n",
       " '80160',\n",
       " '80182',\n",
       " '8027',\n",
       " '80488',\n",
       " '80608',\n",
       " '8077',\n",
       " '80878',\n",
       " '81010',\n",
       " '81151',\n",
       " '81303',\n",
       " '81618',\n",
       " '82050',\n",
       " '82242',\n",
       " '82277',\n",
       " '82324',\n",
       " '82468',\n",
       " '83049',\n",
       " '83118',\n",
       " '83222',\n",
       " '83332',\n",
       " '83355',\n",
       " '83370',\n",
       " '83383',\n",
       " '83435',\n",
       " '83600',\n",
       " '83738',\n",
       " '84',\n",
       " '84025',\n",
       " '84122',\n",
       " '84128',\n",
       " '84199',\n",
       " '85',\n",
       " '850',\n",
       " '85023',\n",
       " '85069',\n",
       " '85222',\n",
       " '85233',\n",
       " '8552',\n",
       " '85555',\n",
       " '86021',\n",
       " '861',\n",
       " '864233',\n",
       " '86688',\n",
       " '86888',\n",
       " '87021',\n",
       " '87066',\n",
       " '87070',\n",
       " '87077',\n",
       " '87121',\n",
       " '87131',\n",
       " '8714714',\n",
       " '872',\n",
       " '87239',\n",
       " '87575',\n",
       " '8800',\n",
       " '88039',\n",
       " '88066',\n",
       " '88088',\n",
       " '88222',\n",
       " '88600',\n",
       " '88800',\n",
       " '88888',\n",
       " '89034',\n",
       " '89070',\n",
       " '89080',\n",
       " '89105',\n",
       " '89123',\n",
       " '89545',\n",
       " '89555',\n",
       " '89693',\n",
       " '8am',\n",
       " '8lb',\n",
       " '8p',\n",
       " '8pm',\n",
       " '8th',\n",
       " '8wp',\n",
       " '900',\n",
       " '910',\n",
       " '9153',\n",
       " '9280114',\n",
       " '92h',\n",
       " '930',\n",
       " '945',\n",
       " '946',\n",
       " '95',\n",
       " '9758',\n",
       " '97n7qp',\n",
       " '98321561',\n",
       " '99',\n",
       " '9996',\n",
       " '9ae',\n",
       " '9am',\n",
       " '9ja',\n",
       " '9pm',\n",
       " '9t',\n",
       " '9th',\n",
       " '____',\n",
       " 'a30',\n",
       " 'aa',\n",
       " 'aah',\n",
       " 'aaniye',\n",
       " 'aathi',\n",
       " 'ab',\n",
       " 'abbey',\n",
       " 'abdomen',\n",
       " 'abeg',\n",
       " 'abel',\n",
       " 'aberdeen',\n",
       " 'abi',\n",
       " 'ability',\n",
       " 'abiola',\n",
       " 'abj',\n",
       " 'able',\n",
       " 'abnormally',\n",
       " 'about',\n",
       " 'above',\n",
       " 'absence',\n",
       " 'absolutely',\n",
       " 'absolutly',\n",
       " 'abstract',\n",
       " 'abt',\n",
       " 'abta',\n",
       " 'aburo',\n",
       " 'abuse',\n",
       " 'abusers',\n",
       " 'ac',\n",
       " 'academic',\n",
       " 'acc',\n",
       " 'accent',\n",
       " 'accenture',\n",
       " 'accept',\n",
       " 'access',\n",
       " 'accessible',\n",
       " 'accidant',\n",
       " 'accident',\n",
       " 'accidentally',\n",
       " 'accomodate',\n",
       " 'accomodations',\n",
       " 'accordin',\n",
       " 'accordingly',\n",
       " 'account',\n",
       " 'accounting',\n",
       " 'accounts',\n",
       " 'accumulation',\n",
       " 'achan',\n",
       " 'ache',\n",
       " 'acknowledgement',\n",
       " 'acl03530150pm',\n",
       " 'acnt',\n",
       " 'aco',\n",
       " 'across',\n",
       " 'act',\n",
       " 'acted',\n",
       " 'actin',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'activ8',\n",
       " 'activate',\n",
       " 'active',\n",
       " 'activities',\n",
       " 'actor',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'ad',\n",
       " 'adam',\n",
       " 'add',\n",
       " 'added',\n",
       " 'addicted',\n",
       " 'addie',\n",
       " 'adding',\n",
       " 'address',\n",
       " 'adewale',\n",
       " 'adi',\n",
       " 'adjustable',\n",
       " 'admin',\n",
       " 'administrator',\n",
       " 'admirer',\n",
       " 'admission',\n",
       " 'admit',\n",
       " 'adore',\n",
       " 'adoring',\n",
       " 'adp',\n",
       " 'adress',\n",
       " 'adrink',\n",
       " 'ads',\n",
       " 'adsense',\n",
       " 'adult',\n",
       " 'adults',\n",
       " 'advance',\n",
       " 'adventure',\n",
       " 'adventuring',\n",
       " 'advice',\n",
       " 'advise',\n",
       " 'advising',\n",
       " 'advisors',\n",
       " 'aeronautics',\n",
       " 'aeroplane',\n",
       " 'affair',\n",
       " 'affairs',\n",
       " 'affection',\n",
       " 'affectionate',\n",
       " 'affections',\n",
       " 'affidavit',\n",
       " 'afford',\n",
       " 'afghanistan',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'aft',\n",
       " 'after',\n",
       " 'afternon',\n",
       " 'afternoon',\n",
       " 'afternoons',\n",
       " 'aftr',\n",
       " 'ag',\n",
       " 'again',\n",
       " 'against',\n",
       " 'agalla',\n",
       " 'age',\n",
       " 'age16',\n",
       " 'age23',\n",
       " 'agency',\n",
       " 'agent',\n",
       " 'agents',\n",
       " 'ages',\n",
       " 'agidhane',\n",
       " 'aging',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'ah',\n",
       " 'aha',\n",
       " 'ahead',\n",
       " 'ahhh',\n",
       " 'ahhhh',\n",
       " 'ahmad',\n",
       " 'aig',\n",
       " 'aight',\n",
       " 'ain',\n",
       " 'aint',\n",
       " 'air',\n",
       " 'air1',\n",
       " 'airport',\n",
       " 'airtel',\n",
       " 'aiya',\n",
       " 'aiyah',\n",
       " 'aiyar',\n",
       " 'aiyo',\n",
       " 'ajith',\n",
       " 'ak',\n",
       " 'aka',\n",
       " 'al',\n",
       " 'alaikkum',\n",
       " 'alaipayuthe',\n",
       " 'albi',\n",
       " 'album',\n",
       " 'alcohol',\n",
       " 'aldrine',\n",
       " 'alert',\n",
       " 'alerts',\n",
       " 'aletter',\n",
       " 'alex',\n",
       " 'alfie',\n",
       " 'algarve',\n",
       " 'algebra',\n",
       " 'algorithms',\n",
       " 'alian',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'allah',\n",
       " 'allalo',\n",
       " 'allday',\n",
       " 'alle',\n",
       " 'allo',\n",
       " 'allow',\n",
       " 'allowed',\n",
       " 'allows',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'alot',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'alrite',\n",
       " 'also',\n",
       " 'alter',\n",
       " 'alternative',\n",
       " 'although',\n",
       " 'alto18',\n",
       " 'aluable',\n",
       " 'alwa',\n",
       " 'always',\n",
       " 'alwys',\n",
       " 'am',\n",
       " 'amanda',\n",
       " 'amazing',\n",
       " 'ambitious',\n",
       " 'american',\n",
       " 'ami',\n",
       " 'amk',\n",
       " 'amla',\n",
       " 'amma',\n",
       " 'ammo',\n",
       " 'amnow',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amore',\n",
       " 'amount',\n",
       " 'amp',\n",
       " 'amplikater',\n",
       " 'amrca',\n",
       " 'ams',\n",
       " 'amt',\n",
       " 'amused',\n",
       " 'amy',\n",
       " 'an',\n",
       " 'ana',\n",
       " 'anal',\n",
       " 'anand',\n",
       " 'and',\n",
       " 'anderson',\n",
       " 'andrews',\n",
       " 'andros',\n",
       " 'angels',\n",
       " 'angry',\n",
       " 'animal',\n",
       " 'animation',\n",
       " 'anjola',\n",
       " 'annie',\n",
       " 'anniversary',\n",
       " 'annoncement',\n",
       " 'announced',\n",
       " 'announcement',\n",
       " 'annoyin',\n",
       " 'annoying',\n",
       " 'anonymous',\n",
       " 'anot',\n",
       " 'another',\n",
       " 'ans',\n",
       " 'ansr',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'answerin',\n",
       " 'answering',\n",
       " 'answers',\n",
       " 'answr',\n",
       " 'antelope',\n",
       " 'antha',\n",
       " 'anthony',\n",
       " 'anti',\n",
       " 'antibiotic',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anythiing',\n",
       " 'anythin',\n",
       " 'anything',\n",
       " 'anytime',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'aom',\n",
       " 'apart',\n",
       " 'apartment',\n",
       " 'apes',\n",
       " 'aphex',\n",
       " 'apnt',\n",
       " 'apo',\n",
       " 'apologetic',\n",
       " 'apologise',\n",
       " 'apologize',\n",
       " 'app',\n",
       " 'apparently',\n",
       " 'appeal',\n",
       " 'appear',\n",
       " 'appendix',\n",
       " 'applausestore',\n",
       " 'applebees',\n",
       " 'apples',\n",
       " 'apply',\n",
       " ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make this into a dataframe -but it's really large\n",
    "df= pd.DataFrame(X_train_vectorized.toarray(), columns=cvect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to manage the size, add additional parameters to the countvectorizer:\n",
    "\n",
    "cvect2 = CountVectorizer(stop_words='english', max_features=500)\n",
    "# stop words excludes words that don't mean anything - the, it, at etc\n",
    "# max features limits to teh most common 500 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorized2 = cvect2.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2= pd.DataFrame(X_train_vectorized2.toarray(), columns=cvect2.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will will just transform the test - not fit\n",
    "X_test_vectorized = cvect2.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate a logistic regression estimator- this is a classification problem, so we could \n",
    "# have used KNN, logistic regression, or decision trees\n",
    "lgr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fir on vectorized training data\n",
    "lgr.fit(X_train_vectorized2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9873175400813592"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgr.score(X_train_vectorized2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9770279971284996"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgr.score(X_test_vectorize, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.865937\n",
       "spam    0.134063\n",
       "Name: spam, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the baseline. So we're at least doing better than that!\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5182677 ,  0.07780361,  0.88587794,  1.02862686,  0.88687662,\n",
       "         1.08305093,  0.3612345 ,  0.17246992,  2.25878932,  0.56863936,\n",
       "         0.7378356 ,  1.18503667,  0.42542975,  0.24929651,  0.34640318,\n",
       "         0.11869043,  1.94408496,  0.90391011,  0.59359128,  0.89792513,\n",
       "         0.76632536, -0.15876073, -0.34741149,  1.0282181 , -0.53548162,\n",
       "        -0.18513611, -0.34507671, -0.27338362, -0.23351989, -0.71880748,\n",
       "        -0.16870081,  0.28971677,  0.62042597, -0.29843707, -0.75473379,\n",
       "         0.08037753,  0.58371316,  1.15489101,  0.36591538,  1.00655455,\n",
       "         1.26995376, -0.15508011, -0.22456728,  0.30976008,  0.06437246,\n",
       "         0.29665407, -0.3045126 , -0.38323333, -0.03716611, -0.38664391,\n",
       "         0.20330831,  0.14024517, -0.06436504, -0.60978921,  0.40312931,\n",
       "         0.36479306, -0.1411452 ,  0.78890597, -0.17570199,  0.07962368,\n",
       "        -0.32358893, -0.15617987, -0.06617278, -0.46080455, -0.24452967,\n",
       "         0.91949319,  0.49132103,  0.36433934,  0.51116511,  0.0480654 ,\n",
       "         0.91667716, -0.44605588, -0.05474071, -0.10348816, -0.17585184,\n",
       "         0.94246401, -0.14916718,  0.54194788, -0.18083161,  1.88875867,\n",
       "         0.58596342, -0.23518616,  1.92082595, -0.39443811, -0.0207574 ,\n",
       "         0.42928366,  0.96820527,  1.19263528,  0.74113014, -0.30546739,\n",
       "         0.02438307,  1.32773268, -0.61526775,  1.00224533, -0.36976456,\n",
       "         1.03002896,  0.66463356, -0.30422465, -0.59405778,  1.35595737,\n",
       "        -0.20015471,  0.62408152,  1.8135715 , -0.83002873, -0.22301157,\n",
       "        -0.37948456,  0.27704846,  1.74897634,  0.38187474,  0.43056167,\n",
       "        -0.24429342, -0.01243842,  0.86902775, -0.22838778, -0.13701798,\n",
       "        -0.15672533, -0.5903718 , -0.29871614, -0.35066529, -0.23385515,\n",
       "        -0.24523602, -0.21867602, -0.64524055, -0.50219927,  0.19231361,\n",
       "         0.39579142,  0.86294246, -0.13298795, -0.14231323, -0.17649682,\n",
       "         0.95646994, -0.19582502, -0.27029909, -0.29924448, -0.28526339,\n",
       "        -0.18732825, -0.1983047 ,  0.6681858 ,  0.83955553,  0.04614862,\n",
       "         0.5644405 , -0.28032515, -0.31466999, -0.01254512, -0.37473713,\n",
       "         0.12533316, -0.34505917, -0.62979433, -0.2258094 , -0.21776567,\n",
       "         0.30406525, -0.27598585,  1.48855626,  0.66857609, -0.13847244,\n",
       "        -0.0968717 , -0.70740369, -0.42002679, -0.23429979,  0.12228097,\n",
       "         0.97686505, -0.38468205,  0.63299991,  0.102625  , -0.11787114,\n",
       "        -0.2318446 , -0.14088583, -0.24043172, -0.28720961, -0.58688611,\n",
       "        -0.32463048, -0.82550982, -0.07340327, -0.69781989, -1.65220343,\n",
       "         0.62918064, -0.51440047,  0.25949254, -0.2436307 , -0.39636179,\n",
       "        -0.15165961, -0.4022523 , -0.13540419,  0.53663982, -0.16368515,\n",
       "        -0.22411509, -0.82993543, -0.52538967, -0.35453624, -0.19123373,\n",
       "        -0.33419301, -0.22701171,  0.07941994, -0.04594433,  0.61831213,\n",
       "         1.32092353, -0.80369573, -0.21189677,  0.65362526, -0.80103772,\n",
       "        -0.61785819,  0.62086535, -0.21497845, -0.12486509, -0.40257267,\n",
       "         1.68278909, -0.26957512, -0.1185515 , -0.75225025, -0.36966557,\n",
       "        -0.24478103,  0.84375915, -0.22626067, -0.54997525,  0.20529875,\n",
       "        -0.24636107, -0.09835243, -0.28307497, -0.24132954, -0.07972782,\n",
       "        -0.19024945,  1.2093834 , -0.33317382, -0.23306781, -0.87494425,\n",
       "         0.58593435,  0.12289355, -0.40534134,  0.13401797, -0.25926822,\n",
       "        -0.17563947, -0.2112008 , -0.58877308, -0.07387545, -0.23783514,\n",
       "        -0.5802978 , -0.07631885,  1.30589214,  0.86890688,  1.00919819,\n",
       "        -0.96215456, -0.56227767, -0.4300922 ,  0.24407902,  0.84240227,\n",
       "        -0.79358864, -0.32198348, -0.39152316, -0.06441382, -0.31224581,\n",
       "        -0.13677999, -1.64018573, -0.01632719, -0.33062355,  0.52768975,\n",
       "        -0.75541748,  0.54996412, -0.16596224, -0.13457821, -0.38160533,\n",
       "        -0.16072152, -0.13855352, -0.45177005, -0.17900083, -0.27064942,\n",
       "         1.71708538,  1.03864166,  2.09271323,  0.06082405,  0.98159551,\n",
       "         0.59514657, -0.20115259,  0.16806345,  0.09235516, -0.58037747,\n",
       "         0.93428054,  1.78940493, -0.16859018,  0.0189049 , -0.48096994,\n",
       "        -0.55607756,  0.44347534,  1.09787871,  0.27133605,  0.81549022,\n",
       "        -0.06404319,  0.69147599,  0.41032276,  1.56107834,  0.59042113,\n",
       "        -0.49529736,  0.43619237, -0.10779122,  0.76601429, -0.18398649,\n",
       "        -0.14461578,  0.69549428, -0.07948565,  0.5554248 ,  0.01838418,\n",
       "        -0.44507938, -0.90332312, -0.29074681, -0.15402183, -0.03186458,\n",
       "        -0.22625938,  0.97763743,  1.0141152 ,  1.9236114 , -0.29173118,\n",
       "        -0.14560565, -0.33354647, -0.17919765, -0.24130166, -1.33125003,\n",
       "        -0.420193  ,  0.44692439,  0.26518051,  0.12306142,  0.34332902,\n",
       "        -0.45650164, -0.23567235,  1.39588591,  0.38417735, -0.83538941,\n",
       "        -0.24209262, -0.30317313,  0.13531248,  1.00597577,  0.59568968,\n",
       "         0.72298689, -0.09753573,  0.81077688, -0.28438245,  1.11914233,\n",
       "         1.1476064 , -0.25714195, -0.54991105, -0.09239653,  0.62332718,\n",
       "        -0.23729373,  0.89062561, -0.14051993, -0.23627164,  0.01007928,\n",
       "        -0.62195697,  1.15846508, -0.50885488,  1.78470282, -0.69899294,\n",
       "         1.66893696, -0.11305311, -0.30934251, -0.32542944, -0.19439868,\n",
       "         0.50406204, -0.63056567, -0.20658544,  0.48930884, -0.26470579,\n",
       "        -0.43603183, -0.1880456 , -0.12319361, -0.38411609, -0.11341698,\n",
       "        -0.32108135,  0.92797885,  0.7078194 , -0.14968977,  2.2000919 ,\n",
       "        -0.14417888,  1.20734385, -0.24719433, -0.32710117, -0.18671855,\n",
       "         0.34632663,  0.58605685, -0.68340881, -0.2684396 , -0.46331338,\n",
       "        -0.32706269, -0.50107923, -0.33383707, -0.17774231,  1.12867892,\n",
       "        -0.09835347, -0.22286057, -0.93887969, -0.11113304,  1.60031761,\n",
       "         0.41187511,  0.19785148,  1.08779836,  0.68225562,  1.46135351,\n",
       "        -0.27253984,  0.0125681 , -0.13386233, -0.62744445, -0.15861059,\n",
       "         0.86955044, -0.34725548, -0.21313268, -0.21217638, -0.36314516,\n",
       "         1.6979767 ,  0.11657327, -0.16594298,  0.24533503, -0.28736587,\n",
       "        -0.37718171, -0.07384226, -0.09627575, -0.3693929 , -0.1930351 ,\n",
       "        -0.31511082, -0.22264513, -0.47504636, -0.19825147, -0.20214535,\n",
       "         0.40015559, -0.37715298, -0.26235625, -0.08965572,  0.45997595,\n",
       "        -0.35112466, -0.31639921, -0.03743607,  1.23413976,  0.89548138,\n",
       "        -0.36486619, -0.2253747 , -0.16679742, -0.22541133, -0.22506122,\n",
       "         0.18742481, -0.11820585,  0.07814887, -0.10039041, -0.25517018,\n",
       "         1.04992098,  2.20091861,  2.93539283,  0.59728281,  0.03740809,\n",
       "         0.52108373,  1.70842911,  0.45912216,  0.3237981 , -0.00376549,\n",
       "         1.27954275,  0.90186514,  0.08835974, -0.37180947,  1.13028055,\n",
       "        -0.34084715,  0.38883852, -0.4415129 ,  0.88096246, -0.25956476,\n",
       "         0.06687132,  0.39965423, -0.68516674, -0.38776496, -0.43818093,\n",
       "        -0.8267847 ,  0.68333829, -0.06887133,  0.20272965, -0.46702685,\n",
       "        -0.35182844,  0.22093932,  0.00712496, -0.13862359, -0.22749308,\n",
       "        -0.31457693,  1.17092895, -0.13388731,  0.32077181,  1.59284825,\n",
       "        -0.32092834,  0.15934361, -0.23969027, -0.3830254 , -0.51122968,\n",
       "        -0.4582574 , -0.27495567,  0.35987637,  0.28154181,  1.56583985,\n",
       "         0.71711636,  0.31686433, -0.44835409, -0.672511  , -0.1219318 ,\n",
       "         0.23169344, -0.89552039, -0.11730176,  0.02454111, -0.39215392]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what mattered? what words drove the classifications? We can look at the coefficients to see\n",
    "# which words were the most important\n",
    "lgr.coef_\n",
    "# this is a 500 2d array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = pd.DataFrame(lgr.coef_[0], columns = ['coef'])\n",
    "feature_df['word'] = cvect2.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>2.935393</td>\n",
       "      <td>uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.258789</td>\n",
       "      <td>150p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>2.200919</td>\n",
       "      <td>txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>2.200092</td>\n",
       "      <td>service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>2.092713</td>\n",
       "      <td>min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.944085</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>1.923611</td>\n",
       "      <td>order</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1.920826</td>\n",
       "      <td>claim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1.888759</td>\n",
       "      <td>chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1.813572</td>\n",
       "      <td>customer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         coef      word\n",
       "442  2.935393        uk\n",
       "8    2.258789      150p\n",
       "441  2.200919       txt\n",
       "369  2.200092   service\n",
       "267  2.092713       min\n",
       "16   1.944085        50\n",
       "308  1.923611     order\n",
       "82   1.920826     claim\n",
       "79   1.888759      chat\n",
       "102  1.813572  customer"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the 10 largest coefficients\n",
    "feature_df.nlargest(10, 'coef')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline and Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use a pipeline to simplify the preprocessing of the data\n",
    "# we use a gridsearch to find the best combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pipeline to do count vectorization and logistic regression.\n",
    "# the order you list in your pipeline matters- the pipelin with start with a transformer and end \n",
    "# with an estimator. You can have as many transformers as you'd like, but the last step has to \n",
    "# be an estimator. \n",
    "# we could just fit the pipeline- no cross validation\n",
    "# the pipeline is doing the transformation and building the model in one step - then\n",
    "# you can cross validation the 5 diff models of each of the different parameter combos \n",
    "# listed in the parameters sectoin\n",
    "# the cross valudation will occur on each of the model combinations\n",
    "# then we can choose the best model based on the mean\n",
    "pipe = Pipeline([('count_vectorizer', CountVectorizer()),\n",
    "                ('lgr', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is where we insert the parameters we'd like to try out.\n",
    "# the double underscores are necessary to separate the parameter from the name\n",
    "params = {'count_vectorizer__max_features' : [100, 500, 1000, 2000],\n",
    "          'count_vectorizer__stop_words' : ['english', None]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('count_vectorizer', CountVectorizer()),\n",
       "                ('lgr', LogisticRegression())])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9976070830342187"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9784637473079684"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is overfit becaue the score on the train is higher than the test. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will take each model, use the params, and do cross validation on 5 slices of data\n",
    "grid = GridSearchCV(pipe, param_grid=params, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('count_vectorizer', CountVectorizer()),\n",
       "                                       ('lgr', LogisticRegression())]),\n",
       "             param_grid={'count_vectorizer__max_features': [100, 500, 1000,\n",
       "                                                            2000],\n",
       "                         'count_vectorizer__stop_words': ['english', None]})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this does all of the transformation and the cross validation in one step!!\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count_vectorizer__max_features': 2000, 'count_vectorizer__stop_words': None}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9813359883104604"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9798994974874372"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({'a',\n",
       "           'about',\n",
       "           'above',\n",
       "           'across',\n",
       "           'after',\n",
       "           'afterwards',\n",
       "           'again',\n",
       "           'against',\n",
       "           'all',\n",
       "           'almost',\n",
       "           'alone',\n",
       "           'along',\n",
       "           'already',\n",
       "           'also',\n",
       "           'although',\n",
       "           'always',\n",
       "           'am',\n",
       "           'among',\n",
       "           'amongst',\n",
       "           'amoungst',\n",
       "           'amount',\n",
       "           'an',\n",
       "           'and',\n",
       "           'another',\n",
       "           'any',\n",
       "           'anyhow',\n",
       "           'anyone',\n",
       "           'anything',\n",
       "           'anyway',\n",
       "           'anywhere',\n",
       "           'are',\n",
       "           'around',\n",
       "           'as',\n",
       "           'at',\n",
       "           'back',\n",
       "           'be',\n",
       "           'became',\n",
       "           'because',\n",
       "           'become',\n",
       "           'becomes',\n",
       "           'becoming',\n",
       "           'been',\n",
       "           'before',\n",
       "           'beforehand',\n",
       "           'behind',\n",
       "           'being',\n",
       "           'below',\n",
       "           'beside',\n",
       "           'besides',\n",
       "           'between',\n",
       "           'beyond',\n",
       "           'bill',\n",
       "           'both',\n",
       "           'bottom',\n",
       "           'but',\n",
       "           'by',\n",
       "           'call',\n",
       "           'can',\n",
       "           'cannot',\n",
       "           'cant',\n",
       "           'co',\n",
       "           'con',\n",
       "           'could',\n",
       "           'couldnt',\n",
       "           'cry',\n",
       "           'de',\n",
       "           'describe',\n",
       "           'detail',\n",
       "           'do',\n",
       "           'done',\n",
       "           'down',\n",
       "           'due',\n",
       "           'during',\n",
       "           'each',\n",
       "           'eg',\n",
       "           'eight',\n",
       "           'either',\n",
       "           'eleven',\n",
       "           'else',\n",
       "           'elsewhere',\n",
       "           'empty',\n",
       "           'enough',\n",
       "           'etc',\n",
       "           'even',\n",
       "           'ever',\n",
       "           'every',\n",
       "           'everyone',\n",
       "           'everything',\n",
       "           'everywhere',\n",
       "           'except',\n",
       "           'few',\n",
       "           'fifteen',\n",
       "           'fifty',\n",
       "           'fill',\n",
       "           'find',\n",
       "           'fire',\n",
       "           'first',\n",
       "           'five',\n",
       "           'for',\n",
       "           'former',\n",
       "           'formerly',\n",
       "           'forty',\n",
       "           'found',\n",
       "           'four',\n",
       "           'from',\n",
       "           'front',\n",
       "           'full',\n",
       "           'further',\n",
       "           'get',\n",
       "           'give',\n",
       "           'go',\n",
       "           'had',\n",
       "           'has',\n",
       "           'hasnt',\n",
       "           'have',\n",
       "           'he',\n",
       "           'hence',\n",
       "           'her',\n",
       "           'here',\n",
       "           'hereafter',\n",
       "           'hereby',\n",
       "           'herein',\n",
       "           'hereupon',\n",
       "           'hers',\n",
       "           'herself',\n",
       "           'him',\n",
       "           'himself',\n",
       "           'his',\n",
       "           'how',\n",
       "           'however',\n",
       "           'hundred',\n",
       "           'i',\n",
       "           'ie',\n",
       "           'if',\n",
       "           'in',\n",
       "           'inc',\n",
       "           'indeed',\n",
       "           'interest',\n",
       "           'into',\n",
       "           'is',\n",
       "           'it',\n",
       "           'its',\n",
       "           'itself',\n",
       "           'keep',\n",
       "           'last',\n",
       "           'latter',\n",
       "           'latterly',\n",
       "           'least',\n",
       "           'less',\n",
       "           'ltd',\n",
       "           'made',\n",
       "           'many',\n",
       "           'may',\n",
       "           'me',\n",
       "           'meanwhile',\n",
       "           'might',\n",
       "           'mill',\n",
       "           'mine',\n",
       "           'more',\n",
       "           'moreover',\n",
       "           'most',\n",
       "           'mostly',\n",
       "           'move',\n",
       "           'much',\n",
       "           'must',\n",
       "           'my',\n",
       "           'myself',\n",
       "           'name',\n",
       "           'namely',\n",
       "           'neither',\n",
       "           'never',\n",
       "           'nevertheless',\n",
       "           'next',\n",
       "           'nine',\n",
       "           'no',\n",
       "           'nobody',\n",
       "           'none',\n",
       "           'noone',\n",
       "           'nor',\n",
       "           'not',\n",
       "           'nothing',\n",
       "           'now',\n",
       "           'nowhere',\n",
       "           'of',\n",
       "           'off',\n",
       "           'often',\n",
       "           'on',\n",
       "           'once',\n",
       "           'one',\n",
       "           'only',\n",
       "           'onto',\n",
       "           'or',\n",
       "           'other',\n",
       "           'others',\n",
       "           'otherwise',\n",
       "           'our',\n",
       "           'ours',\n",
       "           'ourselves',\n",
       "           'out',\n",
       "           'over',\n",
       "           'own',\n",
       "           'part',\n",
       "           'per',\n",
       "           'perhaps',\n",
       "           'please',\n",
       "           'put',\n",
       "           'rather',\n",
       "           're',\n",
       "           'same',\n",
       "           'see',\n",
       "           'seem',\n",
       "           'seemed',\n",
       "           'seeming',\n",
       "           'seems',\n",
       "           'serious',\n",
       "           'several',\n",
       "           'she',\n",
       "           'should',\n",
       "           'show',\n",
       "           'side',\n",
       "           'since',\n",
       "           'sincere',\n",
       "           'six',\n",
       "           'sixty',\n",
       "           'so',\n",
       "           'some',\n",
       "           'somehow',\n",
       "           'someone',\n",
       "           'something',\n",
       "           'sometime',\n",
       "           'sometimes',\n",
       "           'somewhere',\n",
       "           'still',\n",
       "           'such',\n",
       "           'system',\n",
       "           'take',\n",
       "           'ten',\n",
       "           'than',\n",
       "           'that',\n",
       "           'the',\n",
       "           'their',\n",
       "           'them',\n",
       "           'themselves',\n",
       "           'then',\n",
       "           'thence',\n",
       "           'there',\n",
       "           'thereafter',\n",
       "           'thereby',\n",
       "           'therefore',\n",
       "           'therein',\n",
       "           'thereupon',\n",
       "           'these',\n",
       "           'they',\n",
       "           'thick',\n",
       "           'thin',\n",
       "           'third',\n",
       "           'this',\n",
       "           'those',\n",
       "           'though',\n",
       "           'three',\n",
       "           'through',\n",
       "           'throughout',\n",
       "           'thru',\n",
       "           'thus',\n",
       "           'to',\n",
       "           'together',\n",
       "           'too',\n",
       "           'top',\n",
       "           'toward',\n",
       "           'towards',\n",
       "           'twelve',\n",
       "           'twenty',\n",
       "           'two',\n",
       "           'un',\n",
       "           'under',\n",
       "           'until',\n",
       "           'up',\n",
       "           'upon',\n",
       "           'us',\n",
       "           'very',\n",
       "           'via',\n",
       "           'was',\n",
       "           'we',\n",
       "           'well',\n",
       "           'were',\n",
       "           'what',\n",
       "           'whatever',\n",
       "           'when',\n",
       "           'whence',\n",
       "           'whenever',\n",
       "           'where',\n",
       "           'whereafter',\n",
       "           'whereas',\n",
       "           'whereby',\n",
       "           'wherein',\n",
       "           'whereupon',\n",
       "           'wherever',\n",
       "           'whether',\n",
       "           'which',\n",
       "           'while',\n",
       "           'whither',\n",
       "           'who',\n",
       "           'whoever',\n",
       "           'whole',\n",
       "           'whom',\n",
       "           'whose',\n",
       "           'why',\n",
       "           'will',\n",
       "           'with',\n",
       "           'within',\n",
       "           'without',\n",
       "           'would',\n",
       "           'yet',\n",
       "           'you',\n",
       "           'your',\n",
       "           'yours',\n",
       "           'yourself',\n",
       "           'yourselves'})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is a pre-existing dictionary of stop words that comes from a library called nltk\n",
    "cvect2.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the score here is pretty good- but we could try KNN or Random Forest\n",
    "# also- which metric do we care about? is precision the right one? \n",
    "# we'd likely want to avoid missing spam rather than over categorizing it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Build a model to classify a persons status in WhatsApp.  The data is in the files `Emotion(happy).csv` and `Emotion(angry).csv`.  Join the datasets and use the count vectorizer to transform the data, then compare the performance of different classifiers on the test data.  Finally, perform a grid search over parameters of the `CountVectorizer` to see if adjusting how the text is presented improves the scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
